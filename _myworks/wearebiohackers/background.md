---
chapter: Background
parent: wearebiohackers
flow: 3
---

> “Technology is not neutral. We're inside of what we make, and it's inside of us. We're living in a world of connections—and it matters which ones get made and unmade”
> ─ Donna Haraway


To fully understand the biohacker as the collective identity of the DIYbio movement it is necessary to comprehend the central character of the hacker as to give insights into the meanings that the DIYbio movement adopts and adapts from it (see Delfanti, 2013). A short archeology of the hacker culture (§2.1) provides the foundation to understand the attitudes that have built much of our current techno-culture as hackers are considered the heroes of the computer revolution and the architects of the Internet (Levy, 2010). If we consider that technologies are infused with the values of its creators then we must acknowledge that the hacker ethos is embedded in the Internet and propagates within it and has thereby extended into broader realms of social production beyond software and hardware. It is therefore also necessary to appreciate the significance of the hacker ethos and how it is transforming our models of production towards more open and collaborative models (§2.2) and to understand how it is particularly transforming the realm of science (§2.3). This will provide the basis to understand the context that the DIYbio movement emerges from and how the biohacker fits into it (§2.4).


### 2.1 A Hacker Origin
The history and culture of the hacker is best told by Steven Levy (1984) in his book Hackers and the story starts in the late 1950s in MIT with a group of students in the Train Railroad Model Club (TRMC) of the Signals and Power (S&P) Subcommittee who used the word hack to denote a project that not only was constructive but was pleasurable as well. The S&P engineers would program telephone dial switches to control the model trains, and creating a clever connection between relays could be considered a hack but to qualify as a true hack “the feat must be imbued with innovation, style, and technical virtuosity” (Levy, 1984, p. 10). The group became increasingly interested in the emerging field of computing but by then these mainframe machines were reserved for authorized technicians who they called the ‘priesthood’ who ‘zealously guarded the machines’. In 1959, the TX-0 computer arrived and was managed under fewer restrictions and allowed the TRMC hackers to use it; they would stalk the computer room waiting for empty slots and would stay late nights when the computer was ‘off-hours’ just to use it. The hackers were not interested in performing complex arithmetical computations, simulations or statistical analysis as did the ‘Officially Sanctioned Users’, instead they just wanted to explore the limits of the machine. They would spend their time punching out computer code to create programming tools, music programs, and simple games. The programs developed by the hackers were freely shared among each other as a way to admire each other's work, build on it, and even improve it.


In the following decades computers were becoming smaller and considerably more affordable with microcomputers, nevertheless they continued to be reserved for professional settings as they were deemed useless anywhere else. By the 1970s, hackers and entrepreneurs started to use computers for commercial applications in the area of arcade and video games such as Pong in 1972 and Space Invaders in 1978 by Atari which launched the computer (and video game) industry into the mainstream market. At this time computer enthusiasts were beginning to create their own personal computing devices. Hackers and hobbyists began meeting in hobby computer clubs to share and trade parts, circuits, and the designs of their inventions. It was the first meeting of the Homebrew Computer Club in 1975 in Silicon Valley that inspired Steve Wozniak to design a personal microcomputer kit (Wozniak, 2007). Wozniak at the time was working at HP but did extra design work for Atari with his friend Steve Jobs who was employed there. Wozniak presented his kit to the club in 1979 and together with Jobs it became the Apple I computer. With seed money from Wozniak selling his car they founded the Apple Computer Company to manufacture and market the Apple I from Job's family garage (Ceruzzi, 2003). Personal computers were becoming no longer a hobby/do-it-yourself activity where hackers manufactured and shared designs with each other, instead they began to compete as computers became a viable consumer product in the market. The subsequent success of Apple II and Macintosh (and similar ventures) were first ignored and disparaged by universities and corporate giants because of their humble garage origins, “but soon the upstarts became the establishment and the union of capital with this fledging science occurred at warp speed” (Conner, 2009, p. 488).


Computers are of course possible because of their hardware components but software is what makes it useful. When computers were mainly mainframe machines with vacuum tubes that filled up an entire room they were very expensive, so they were leased rather than purchased with software and services included (Ceruzzi, 2003). The source-code (as in human-readable computer-commands) was freely supplied and users were able to customize it to their needs and create new programs. It was until 1969 when the US government sued IBM for attempting to monopolize the computer market that as a response IBM ‘unbundled’ the software and services from hardware sales and ceased to share their source-code (Burton, 2002). Software became a new commodity and underwent legal procedures to secure assets through the Intellectual Property Rights (IPR) regime and was deemed patentable by the UK in 1962 and copyrightable by the US in 1974 (for a quick crash-course on IPR see Box 2.1). Software companies soon started to lease software for a fee and restrictions on its use were enforced through contract law prohibiting users to copy, share, reverse engineer or modify the product without permission (check the Box in a Box 2.1). Licensing proprietary software became a multi-billion dollar industry, by no mistake it propelled Bill Gates, the co-founder of Microsoft, as one of the richest people in the world (Perelman, 2003). As closed software (executable binary code without source-code) was starting to become the status quo, rebellious but prodigious hacker Richard M. Stallman saw this as a threat to the communal values of the hacker community and in 1983 he started to work on the GNU project to build a free operating system that anyone would be free to use, copy, and modify.

<div class="ui secondary segment" markdown="1">
**Box 2.1: Crash-Course in Intellectual Property Rights**

Intellectual Property Rights (IPR) are legally recognized exclusive rights to creations that involve authorship, such as music, literature, designs, discoveries, and inventions. Some of these include trademarks, trade secrets, patents, and copyrights. Trademarks™ are designs and symbols that allow a product, company or service to be recognized and distinguished from others. Trade secrets are just that, designs, formulas, and information that are not disclosed to the public. Patents therefore are supposed to work as incentives for inventors to publish their inventions to the public. Patents reserve exclusive rights for 20 years for novel and useful technological inventions to prevent others from commercial use of the invention without permission. The idea of this enforced monopoly control is to recoup initial investment through monopolistic prices and by granting commercial rights to others in exchange for a licensing fee. Obtaining a patent is a lengthy and costly procedure and an application has to be submitted to different countries for approval. Those opposing the use of patents claim that the system has created patent misuse, thickets, trolls, and ambushes. Misuse is the purposeful intent to make patents broad, and thickets refer to the dense number of patents that make it prohibitively expensive to develop new technologies. Trolls are companies in the sole business of licensing, and patent ambush happens when holders allow for technology to develop and suddenly present an essential patent for that technology and sue for infringement. Copyright© restricts use and distribution to the copyright holder for a period of time, usually measured as author's life + 70 years (in the US since 1998). Under the Berne Convention of 1886 signed by most countries, copyright is automatic and does not require application. The idea of copyright is to enable authors to receive financial compensations for their creations. With digital media that can be effortlessly copied infinitely with no additional costs, copyright infringement has become rampant as users share and remix content freely. As a consequence new technologies known as Digital Rights Management (DRM) have been created to control the use of digital content. Those opposing copyright argue that it restricts the free flow of knowledge and culture and therefore hinders their (re)production. For a complete overview on IPR see Richard Stim's book (2014) Patent, Copyright & Trademark.
<div class="ui tertiary segment" markdown="1">
**Box in a Box: End-User License Agreements**

Licensing is not a form of IPR but is a form of maintaining exclusive control through contract law using End-User License Agreements (EULA); those texts you see/read just before you click “I agree to these terms and conditions”. Restrictive EULA's are a form of DRM. Paradoxically, licensing a software—or any digital material for that matter—requires the user to bear a copy which is technically copyright infringement since the licensee holds no ownership over the copy. The US government amended the Copyright Act in 1980 to grant explicit rights for users to use a copy without infringement. For more information on IPR in the digital world see Peter Yu's (2007) Intellectual Property and Information Wealth.
</div>
</div>

#### 2.1.1 Free/Open-Source Software
Stallman, dubbed as the last hacker by Levy, became increasingly discontent with the subsidence of the hacker community to proprietary software which he viewed as “antisocial and unethical” (Stallman, Lessig, & Free Software Foundation (Cambridge, 2010). In 1985 Stallman founded the Free Software Foundation (FSF) as a way to support the development of free software, “Free as in free speech, not as in free beer” Stallman said (2010, pg. 3). The GNU project was growing as developers were hired by the FSF to contribute to the project and as volunteer and paid programmers from industry contributed as well. In 1991 Linus Torvalds, a computer science graduate student from Helsinki, created his own operating system as part of his master thesis and developed the Linux kernel; the missing component for the GNU project. This led to the development of the GNU/Linux operating system which is arguably the most successful and powerful software in the world. Linux runs 81% percent of the total market share in smartphones with Android devices (IDC, 2013) and runs 97% of the top 500 supercomputers in the world (Noyes, 2014); from air traffic control systems, the New York stock exchange, the largest particle physics laboratory in the world (CERN), and even runs various web and cloud services that power Internet giants such as Google, Amazon, Twitter, and Facebook (Amaresh, 2013).


Free software is made possible because of its licensing scheme. In 1989, Stallman with the help of a law professional published the GNU General Public License (GPL). Free Software is distributed under a legal copyright framework that instead of strictly allocating producer control it stresses the rights of the user and guarantees them “the freedom to run, copy, distribute, study, change and improve upon the software” however they seem fit (Stallman et al., 2010, p. 3). More essential is that the GPL holds an EULA (see Box in a Box 2.1) that requires that all subsequent copies and derivatives thereof bare the same license, effectively locking the source-code as unrestricted commons. This ‘viral’ license is called Copyleft as a play on the word Copyright ©. In 1997 Eric Raymond published his book The Cathedral and the Bazaar where he provides a reflective analysis on the different dynamics between organizational models for the production of software—after being intrigued over the success of the Linux system which he attributed to the bazaar model; as self-organized, decentralized, and distributed (over the Internet) based on voluntary cooperation. The bazaar model describes a model of software production where users are treated as co-developers and software programs are continuously developed and released. Raymond postulated that the bazaar resulted in better software essentially because of what he called ‘Linus Law’ encapsulated in the aphorism “given enough eyeballs, all bugs are shallow”, making reference to harnessing the potential of many contributors, a kind of collective intelligence. In 1998 Netscape inspired by the potential superiority of the bazaar model that Raymond suggested released its browser's source-code and it became Mozilla Firefox. This event incited a group of people to suggest a rebranding of Free Software as a strategy to make the model more ‘business-friendly’. The group founded the Open Source Initiative (OSI) in 1998 and suggested the label Open-Source Software (OSS) which appealed to a more pragmatic stand rather than an ideological one like Stallman's Free Software (FS). Indeed Stallman (1998) maintains that FS and OSS have fundamental different values and ways of looking at the world “For the Open Source movement, non-free software is a suboptimal solution. For the Free Software movement, non-free software is a social problem and free software is the solution”, and thus he differentiates them as “OSS is a development methodology; FS is a social movement” (Stallman et al., 2010, p. 84). The term Free/(Libre) Open-Source Software (F(L)OSS)[^3] is often used to encompass both terms. FLOSS can have distinct philosophical origins (moral vs pragmatic) and their definitions on what makes a software free or open-source do vary to the extent that all free software is considered open-source but not vice versa, they both however do create copyleft software and use a development methodology—more often than not—of a bazaar-like model.

#### 2.1.2 Hacking Principles
The hacker mentality was instrumental in developing the hardware and the software industry. Hackers more than just eager tech-enthusiasts shared a communal set of unstated pragmatic and aesthetic principles which was defined in 1984 in Levy's book Hackers as the hacker ethic:

Hacker Ethic
- Access to computers—and anything which might teach you something about the way the world works—
should be unlimited and total. Always yield to the hands-on imperative!
- All information should be free.
- Mistrust authority—promote decentralization.
- Hackers should be judged by their hacking, not bogus criteria such as degrees, age, race or position.[^4]
- You can create art and beauty on a computer.
- Computers can change your life for the better.


Levy tried to illustrate how hackers distasted restrictions and permissions of all kind, especially bureaucratic ones. They firmly believed that access to ‘things’ is fundamental and that information is a key component to ‘do’ anything. Hacking was something that was to be appreciated for its visionary quality, quirky styles, and innovative techniques, and so hackers should be admired for their feats alone. They also believed that computers could bring joy for personal satisfaction or communal fulfillments. It is easy to see how the hacker ethic influenced the organizational structure of the FLOSS development model for its open and horizontal features (like a bazaar).

The hacker ethic has also been revised by Pekka Himanen in his book The Hacker Ethic and the Spirit of the Information Age (2001). In the prologue written by Torvalds, he reclaims ‘Linus's Law’ from Raymond and suggests that motivations escalate from survival, social life to Entertainment with a capital E; “the kind that gives your life meaning” (Himanen, 2001, p. xvi). Torvalds posits that hackers do things because they find them interesting and they want to share this interesting thing with others fulfilling both the entertainment part from doing something interesting and the social part from sharing it with others. Himanen goes on to argue that the hacker ethic represents a different work ‘attitude’ from Max Weber's The Protestant Ethic and the Spirit of Capitalism (2001). He discusses the current domination of the protestant work ethic where work is seen as a dutiful necessity that is motivated by money and strives towards optimality, consequently work hours are separate from play (leisure) hours. This ethic as explained by Weber is central in the capitalist system. On the other hand hackers value above all passion, creativity, and creating value.


For Himanen hackers are not anti-capitalistic, he explains that for hackers money are the means (rather than ends) to gain freedom and more leisure time. However, George Dafermos and Johan Söderberg, argue that the model of FLOSS explicitly organizes labor in an alternative model based on common ownership of the means of production (libre access to source-code), volunteer participation (free association), and self-expression (directed by passion and value-creation), they thus argue that the hackers personify the struggle against the informational capitalism of IPR and to the organized waged labor of centralized market-oriented hierarchies (Dafermos & Söderberg, 2009). ￹ Gabriela Coleman and Alex Golub (2008) maintain that the hacker ethic applied in the genre of FLOSS can then be understood as a combination of different moral principles of liberalism. The philosophy of FS invokes issues of freedom and access to knowledge and information that invoke “virtues of sharing and pedagogy” (ibid p. 26). OSS advocates for freedom as well as efficiency in the market, as Raymond (1998) suggested that open-source creates ‘better’ software because the motivations of hackers rely on joy and recognition rather than based on a salary-incentive. Coleman and Golub also maintain FLOSS ensues viewing work as a creative form of expression and carries an awareness of connection with a community; of acknowledging their contributions to a commons that can be freely used, and reused (ibid). The hacker ethic and FLOSS can be then be understood as a new ethos towards the economic, social, and cultural arrangements of the production of valuable goods, and this challenge could extend beyond the realm of computers, as Levy (1984, p. 37) proposed in his book:

> "And wouldn’t everyone benefit even more by approaching the world with the same inquisitive intensity, skepticism toward bureaucracy, openness to creativity, unselfishness in sharing accomplishments, urge to make improvements, and desire to build as those who followed the Hacker Ethic?"

Levy's book was fundamental in describing the hacker community and culture, it gave it a history, an identity and an ethic. During the next years hackers were starting to form more formal collectives, some of these would become the seeds for hacktivism; a different genre (application) of the hacker liberal values (G. A. Coleman & Golub, 2008). Eventually hackers started to create different kinds of communities; public communities in dedicated urban spaces known as hackerspaces.


#### 2.1.3 Hackerspaces
Nick Farr (2009) has categorized the emergence of hackerspaces in a ‘Toffleresque’[^5] framework of successive waves. He identifies the first wave in the early 90s with the establishment of hackerspaces in the US. The second wave emerged in Europe with spaces such as C4 established in 1994 and c-base in 1995 in Germany. These spaces began to shape a more sustainable model for an open and more formally organized space gaining “recognition from the government and respect from the public by living and applying the Hacker ethic in their efforts” (Farr, 2009). The third wave of hackerspaces came in 2007, after North American hackers organized a trip called ‘Hackers on a Plane’ to tour around European hackerspaces and to attend the Chaos Communication Camp, which is an international meeting of hackers organized every four years since 1999 by one of the oldest and most recognized hacker clubs, the Chaos Computer Club (CCC) in Berlin founded in 1981. The hackers inspired, upon returning to America established their own hackerspaces such as NYC resistor in New York City and Noisebridge in San Francisco (Borland, 2007). This third wave of hackerspaces represents the current generation of hackerspaces. Hackerspaces.org founded in 2007 acts as the main online hub and presents a comprehensive user-maintained list of all active hackerspaces throughout the world counting 1040 so far and 347 in planning. Hackerspaces define themselves as “community-operated physical places where people can meet and work on their projects”. Jens Ohlig, a pioneer on the early hackerspaces, defined hackerspaces as: “An alternative educational institution, a place where people can learn about technology and science outside the confines of work or school. It's where people build things because they want to, not because they need to make money.” (Newitz, 2009).


The growth of hackerspaces is deeply linked with the development of the Maker culture which vows to the educational model of ‘learning by making’ and to the belief that creating something new and learning new skills is personally enriching and satisfying (Dougherty, 2012). The maker movement has its origins in Make magazine which focuses on DIY[^6] technology projects and publishes instructions and tutorials to make them. Make magazine was founded by Dale Dougherty in 2005 and he initially wanted to name the magazine Hack however his daughter didn't like the name as it sounded too oriented towards programming, instead she suggested calling it Make because “everyone likes making things” (Cavalcanti, 2013). Some DIY individuals and groups have adopted the word maker over the term hacker as they think it better accommodates non-engineers or to avoid the popular pejorative perception of hackers as mischievous cybercriminals (Seckinger, Park, & Gerhard, 2012). Although the term maker, hacker, and tinkerer have subtle differences in meaning they are widely used interchangeably inside the maker/hacker culture (Osborn, 2013).

Along with makerspaces and hackerspaces another similar model emerged around 2005 known as FabLabs which describe themselves as providing “widespread access to modern means of invention”, but can be understood as a global network of small-scale workshops for personal digital fabrication. The concept was developed by the Center for Bits and Atoms at MIT's Media Lab with the intent of empowering under-served communities with technology at a grassroots level (Mikhak et al., 2003). The founding principle of FabLabs is to provide a core set of tools such as 3D printers (adds material), CNC mills (subtracts material), laser and waterjet cutters and so on, that allow individuals to ‘make (almost) anything’. Access to these tools have dramatically reduced the costs of prototyping and production allowing individuals to develop customized products unavailable is the mass-production market. These new grassroots models of—predominantly digital—fabrication have gathered a lot of attention for their potential to encourage user innovation, entrepreneurship, and sustainable alternatives (Smith et. al., 2013). Jarkko Moilanen has noted that even though these communities might use different denominations to classify themselves (hackerspaces, makerspaces, or FabLabs) “they are all mainly concerned about projects led by users and about having an impact on the social environment” (Moilanen, 2013, p. 6). He found they hold similar values of sharing, collaborative work, openness, and transparency (Moilanen, 2012). Moilanen has equated these spaces as a third place as defined by Oldenburg; a place separate from work and home where people develop communal ties.

Fablabs, makerspaces, and hackerspaces are all interlinked models which are open co-working spaces where people socialize, learn, collaborate, and share knowledge, tools, and space (Moilanen, 2013). They also organize international community events such as fairs, festivals, conferences, camps, and hackatons[^7]. These DIY communities have revived the DIY hardware ethic of the original hackers and hobbyists but have surpassed it in scale thanks to the Internet which has greatly facilitated creating and sharing designs, schematics, and ‘how-to’ instructions manuals through digital format. Moreover, they have taken the FLOSS model to physical objects practicing open design and most notably Open-Source Hardware (OSH) which includes sharing design files, schematics, firmware, software, and instructions for manufacturing—all is made free to use and remix under copyleft licenses such as OSH, FLOSS, and CC (Creative Commons c), or OHL (Open Hardware License)[^8]. New successful business models surrounding open-source hardware have emerged such as Arduino, Adafruit, and Sparkfun where the user (consumer) of the product becomes a co-developer and a producer of his own as well.


The hacker ethos in FLOSS and hackerspaces is not an isolated phenomena, instead it should be understood from the transitions of a social/technological paradigm shift caused by Information and Communication Technologies (ICTs) and a cultural/economic paradigm shift of commons-based peer-to-peer production.

### 2.2 The Internet Primer
As our global society transitions into the Information (Digital) Age[^9], we are living under new socio-technical conditions created by the increasing and pervasive use of microelectronics and digital communication networks which have become intrinsically embedded in almost every aspect of our modern human lives. Sociologists Manuel Castells (2000) and Jan van Dijk (2006) have defined this new social (infra)structure as the Network Society where ICTs constitute the integral backbone that maintains and develops our economies, our societies and our cultures. Joi Ito calls this new world the After-Internet (AI) world in contrast to Before-Internet (BI), and says that the AI radically reduced the cost of connectivity and democratized participation to all users of the Internet which enabled a more diverse and greater production/innovation capacity at an unprecedented scale (Ito, 2013). New web technologies of the web 2.010 have furthered reinforced the architecture of participation of the Net as they are designed to support and encourage user-generated content (O’Reilly, 2007), or in other words the work of amateurs. Amateurs in this sense refers to individuals that partake in an activity by the sheer pleasure and satisfaction they get from it rather than for strictly financial or professional gains. As the technologies, skills, and knowledge required for the production and distribution of content become more easily accessible and affordable through new forms of digital media and tools, amateurs have increased the quality of their work enough to compete with larger, hierarchical, professional organizations, such is the case of the blogosphere vs professional publishing (Shirky, 2002). This process has been labeled by Clay Shirky as mass amateurization (2008) and by Charles Leadbeater and Paul Miller (2004) as professional amateurization (Pro-Ams). Shirky analyzes mass amateurization from the media revolution of the Internet as the first medium that has ever combined two-way group communication which has enabled group forming and group action. In the AI world, people can freely share, converse, collaborate and coordinate collective action through the Net. Shirky (2008) maintains that when content can be produced more easily in a networked and participatory environment it undermines the scarcity model of top-down professionally mass-produced content. Consequently, the traditional linear relationship between producer → consumer is disturbed as leisure becomes an active form of production and is no longer passive consumption. As end-users increasingly (co)produce more content, Axel Bruns (2008) suggests the term produser (producer/user) to denote user-led content in a fluid, heterarchical, collaborative commons model like the famous example: Wikipedia (check Box 2.2).

<div class="ui secondary segment" markdown="1">
**Box 2.2: Wikipedia, Wikipedia and Wikipedia**

Wikipedia is cited as one of the most iconic examples of the current shift towards a free, open, decentralized, distributed, and collaborative model of production. Wikipedia is the most popular encyclopedia in the world with a reliability compared to that of traditional encyclopedias (Giles, 2005). The success of Wikipedia is often contrasted with the failure of its predecessor Nupedia, created in 2000 by Jimmy Wales and Larry Sanger. It was to be the first free online encyclopedia in English written by highly qualified expert volunteers and the articles would go under formal peer-review. By the first year they had created 21 articles. Sanger learned about the wiki technology and proposed to attach it to Nupedia as a feeder for discussions and ideas for new articles. They named it Wikipedia and launched it in 2001. By the first year Wikipedia had 18,000 articles. It currently holds over 30 million articles in 287 languages with over 21 million user accounts
</div>

But before Wikipedia there was of course Linux. Raymond (1999) tried to understand FLOSS through the bazaar model; as a permissionless and distributed development model. The socio-economic production model of FLOSS is explained by Yochai Benkler (2002) as Commons-Based Peer-Production (CBPP) which describes how content is created and maintained collectively in a commons by a distributed and decentralized community of peers (users and developers) that contribute freely to a project mostly by intrinsic motivations without the need of hierarchical organizations (firm production) and/or financial compensations (market-based production). Michel Bauwens (2005) calls it Peer-to-Peer (P2P) production and differentiates this model as a new mode of: Production, which is oriented towards use-value (for-benefit) rather than exchange-value (for-profit); Governance, which are peer to peer horizontal hierarchies; and Distribution, which is a shared ownership of tangible and intangible commodities. Bauwens further identifies five key infrastructures required for P2P production: (1) Technological Infrastructure that enables distributed access to capital; (2) Information and Communications Infrastructure that allows autonomous content creation and communication between cooperating agents; (3) Software Infrastructure that produces collaborative tools; (4) Legal Infrastructure that protects creative works from being appropriated; and a (5) Cultural Infrastructure, a type of “cooperative individualism needed to sustain an ethos that enables P2P projects”. Peter Troxler (2010) thus argues hackerspaces can be seen as the result of applying the CBPP model to both immaterial and material goods.


A key issue in CBPP is the turn to viewing information, knowledge, and culture as a commons; as collaborative authorship that is created collectively and cumulatively as opposed to something that is created individually and thus allows for it to be expressed in terms of individual ownership. Lawrence Lessig (2004) in his book Free Culture describes the latter as permission culture referring to the traditional producer-control model that enforces IPR[^11] to restrict the creation of derivative work which he argues discourages innovation and the (re)production of content. Lessig thus advocates for a default free culture where content is freely shared to build and improve upon by changing, modifying or remixing it—or ‘forking’ in software terms. Just like free software, free culture concerns itself with the freedom of ‘produsers’ rather than on the exclusive rights of the producer. These transformations have been widely observed and analyzed by new media theorists such as Henry Jenkins (2009) in his account of participatory culture, which he characterizes by having low barriers for creative expression and civic engagement, support for creating and sharing knowledge, informal mentorships for transferring knowledge and experience, and a sense of people valuing their own contributions and that of others. It started all with Linux, it spread through the web 2.0, it became renown with Wikipedia, and it was transformed “from bits to atoms” by DIY community spaces. All realms of social production have been affected by the AI world, including the production and distribution of science. Unlike other areas of social production like popular culture where the authority over culture is of the folk, science is an established institution with a set of norms that are part of the description of what makes science science.

### 2.3 The Strand of Science
Science can be recognized as both the organized body of knowledge in any area of inquiry (natural or social) and the social processes and activities of obtaining that knowledge (Bhattacherjee, 2012). This body of knowledge has been accumulating for thousands of years, from the Paleolithic Era to the Post-Modern Era—from stone tools to quantum computers. Throughout history, artisans, philosophers, amateur and professional scientists have contributed to this stock of knowledge (Conner, 2009). Science as we recognize it today is the result of the institutionalization and professionalization of science. In the 17th century early scientific societies composed by gentlemen scientists started to emerge and eventually modeled what would become the modern form of science as a body of authority and control over scientific knowledge and practices (ibid.). Modern Science was then defined as a cumulative and collective endeavor that would provide public knowledge and would serve as a modern system for innovation in capitalistic economies (Zilsel, 2000). At the end of WWII a new model for scientific knowledge production emerged that separated scientific inquiry into two different but complementary purposes: knowledge for the sake of knowledge (pure knowledge) in academic science, and knowledge for the sake of profit (practical applications) in industrial science (David, 2005).

#### 2.3.1 Cathedral-Like Science
Academic science was established as a social contract as part of a gift-economy[^12] between professional scientists and society (Vermeir, 2012). Academics require capital means to carry out their research and to sustain themselves financially, these funds are provided by society through the patronage of the state. In exchange for freedom of inquiry professional scientists are expected to openly disclose their knowledge, inventions, and discoveries, and to contribute to higher-education in the case of universities. In return for their ‘gift’ of knowledge they receive recognition and esteem as the material compensation. Robert Merton (1973) maintained that recognition was the reward mechanism for academics rather than money. He explained in his essay The Normative Structure of Science the social norms of science as CUDOS, a mnemonic for: Communalism, which acknowledges scientific knowledge as a public-commons, Universalism means that anyone can contribute equally and knowledge is treated critically equally, Disinterestedness in personal gains and focus in ‘neutral’ science, and Organized Skepticism[^13] signifies how science should be openly reviewed and scrutinized. Industrial scientists on the other hand work inside the market-economy where in exchange for financial compensations they develop knowledge in secrecy protected under IPR to maximize profits through commercial exploitation. Industrial science therefore follow the counter norms of CUDOS which John Ziman (2002) coined as the PLACE norms: Proprietary to denote how knowledge is privatized, Local means it addresses technical problems rather than general understanding, Authoritarian describes how scientists work under managerial hierarchical control, Commissioned means it has a practical goal, and Expert refers to how scientists are hired as problem solvers and not for their curiosity.


The demarcation between academic and industrial science has not always been clearly defined, as “new knowledge produces new practices and vice versa”, hence basic research and technological development (except for cosmology) “in the long run become indistinguishable” (Ziman, 2002, pp. 171– 172). But in the last century academic science no longer strictly follows the idealistic CUDOS norms—if it ever did—but according to Ziman, it follows the PLACE norms and is now a post-academic science which represents the reorganization of science under market principles as research projects are established on the interests of funding agents such as private firms and government departments. Thus academics no longer engage in science as a free exploration but instead science is commissioned under the demands of the sponsors.


Academic research is measured primarily by contributions to peer-review publishing which represents the ultimate form of currency that determines the success of a scientist (Long, 1978). This is exemplified in the aphorism of “Publish or Perish”. Hence publishing has become an ends instead of the means of scientific research. The distribution of scholarly literature belongs to publishers which establish copyright control—in the digital world the copy is licensed not owned. Meanwhile, public funded research has expanded their efforts to appropriate the intellectual capital of knowledge-workers by patenting scientific discoveries for commercial applications (David, 2004). Increased partnerships with private interests have also resulted in developing knowledge in secrecy as academics trade in publishing in academic journals for financial compensations in the form of a job or licensing fees from patenting (Ziman, 2002). The current model of science has thus succumbed to competitive behaviors under a reward system that measures scientific progress through publications and market potentials, where knowledge and information are treated as commodities rather than as public-commons (Vermeir, 2012). Furthermore, the enclosure of these immaterial assets through proprietary regimes create an artificial scarcity that has considerably constrained the free flow of knowledge and is considered to deter the development of Science, Technology and Innovation (STI) (Heller & Eisenberg, 1998). This artificial scarcity however, is hard to justify and maintain in the AI world where information and knowledge can be shared at a near zero marginal cost.

#### 2.3.2 Bazaar-Like Science
ICTs have significantly changed the way science can be produced and distributed. Knowledge and information is more easily stored and shared in digital format and web 2.0 technologies have improved the dialog of science by increasing and facilitating communication and collaboration. This new web-based approach to the organization of science is described as science 2.0 (Waldrop, 2008), or networked science as defined by Michael Nielsen (2012), will require to change the culture of science from a competitive ordeal towards a collaborative one that openly shares scientific content. This shift is being facilitated by the open science movement which is part inspired by the FLOSS development model (Willinsky, 2005).


The open science movement aims to make science more accessible to all levels of society by making scientific knowledge free to use, re-use, and distribute without legal, technical or social restrictions (Open Knowledge Foundation, 2014). Open science advocates for open access (libre and gratis) to scientific literature, primarily scholarly journals but also includes dissertations and books. Open access publication initiatives have been steadily growing and proving to be successful, such as the Public Library of Science (PLOS) project founded in 2000, in which the open access journal of PLOS ONE is now the world's largest journal (Van Noorden, 2013). Open access can also require the non-textual elements of accompanying scientific publications and research and it is sometimes separately addressed as open science data. Open science also promotes new ways of doing research such as publishing the ongoing research process online through digital open notebook science which includes raw data. Another issue that open science advocates is for sharing all of the data obtained, that includes negative results which would otherwise be deemed as ‘unpublishable’. Open science can also encourage the engagement of citizen scientists and amateur scientists—whether they work with, at the edges or beyond mainstream science. Citizen science can exist as an extension to institutionalized science where computing resources and cognitive labor is capitalized (crowdsourced) from the public to produce information and knowledge (Hand, 2010). Examples of these include Folding@Home, a screen-saver that performs protein folding simulations and other types of molecular dynamics, and EteRNA, a web-based game where players solve RNA folding mechanics puzzles. On the other hand, citizen science can emerge as grassroots initiatives; as self-organized and autonomous peer-to-peer communities that engage in the development of STI. One example of a community-led science network is the Do-it-Yourself Biology (DIYbio) community.

### 2.4 A Translation into DIYbio
Amateur biology hit the DIY scene when Make magazine published its special section in Backyard Biology volume 07 in 2006. The section included tutorials on how to freeze and revive a garden snail, how to extract and characterize your own DNA and build a thermal cycler and run PCR[^14] for replication, and how to hack your plants with grafting techniques. In the same issue, the featured profile (proto) was entitled “Garage Biotech: For a safer world, Drew Endy wants everyone to engineer life from the ground up”. The article featured Drew Endy and his latest campaign to promote the growth of garage biotech arguing that the world would be a safer place if engineers could see biology as hackable. The article ends by saying “Endy hopes that, in a few years, biology will be further demystified as just another technology, the price of gene synthesis will become more affordable, and rank amateurs will take on ambitious projects. The bugs and the bees may never be the same again” referring to the introduction the author gives to Endy as pointing to a bumblebee noting it is an editable reproducing machine saying “Why can't I just hack this stuff?” (Parks, 2006).

#### 2.4.1 Biology can be Hackable
Endy is one of the pioneers of synthetic biology (synbio), a burgeoning field that instead of copying genetic parts and pasting them in other organisms (genetic engineering), synbio envisions biological systems as controllable systems that can be engineered with standardized parts and devices that can be modulated and (re)designed from the bottom-up[^15]. The field of synbio has been essential in instilling the sense of understanding living organisms by analogy with electronic devices; cells as the hardware of biology and DNA as the software of life. Endy together with computer scientist Tom Knight at MIT designed the BioBricks DNA assembly standard in 2003 which is commonly explained as Lego-like building blocks (Shetty, Endy, & Knight, 2008). BioBricks are standardized and interchangeable sequences of DNA which are assembled like electronic components into synthetic biological circuits and operated inside living cells. The whole premise of standardized parts in engineering is that the specifications are shared among ‘manufacturers’ to facilitate automation and part re-use. Endy and Knight founded in 2003 the Registry of Standard Biological Parts, an open-access repository for BioBricks that are collectively created and communally shared. Endy and Knight together with Randy Rettberg and Garry Sussman, established a class in 2003 providing ‘hands-on introduction to the design and fabrication of synthetic biological machines’ to undergraduates during MIT's Independent Activities Period (IAP); a “four-week period where students engage in innovative projects that combine learning and fun” (Brown, 2007). The 2004 IAP grew into an intercollegiate summer competition with five schools from the US with the goals to increase interest in synbio research and to foster interdisciplinary collaboration (Campbell, 2005). Thirteen teams participated in the 2005 jamboree and included two international teams from Toronto and Zurich; transforming the jamboree into the international Genetically Engineered Machine (iGEM) competition. The teams received non-hierarchical awards such as ‘Coolest Part’, ‘IKEA Idea Award’, ‘Best “Quantitative” Answer’, and ‘Most Innovative Abuse of Expensive Laboratory Equipment’. The IGEM competition and the BioBricks Registry according to Peter Robbins (2009) have broken traditional paradigms of science by pushing towards with their open-source innovation model, interdisciplinarity and engagement with social concerns such as biosafety and corporate control. IGEM has since expanded to include high-school students (since 2011), entrepreneurs (since 2012), and community labs (for the first time in 2014). The competition has been widely successful, with 246 teams from Europe, Asia, North and Latin America registered to compete in 2014 with projects that focus on the environment, health and medicine, food and nutrition, energy, and new tracks that focus on art and design, policy and practices, software, etc.

#### 2.4.2 Resources get cheaper
Endy's desire for a demystified biology expressed in his proto piece in Make of 2006 was starting to take shape as students were building complex machines, but affordable machines were still lacking at the time. In 2005 the first Next-Generation DNA Sequencing (NGS) technology hit the market with the 454 Life Sciences Genome Sequencer at a price tag of $500K (Perkel, 2006). In 2008 using 454's sequencer the full-genome of James Watson, the co-discoverer of the double helix, was sequenced in about 2-4 months time for a cost between $1-2M (Davies, 2008). NGS was a significant breakthrough compared to other sequencing projects, like Craig Venter's[^16] full-genome sequence took years at a cost of almost $100M, or the international consortium for the Human Genome Project (HGP) which took 13 years and $3B (Bartfai & Lees, 2013).


After the HGP was complete in 2003, George M. Church, a genomics and synthetic biology pioneer, founded the Personal Genome Project (PGP) as an offshoot in 2005 which intends to sequence and openly publish the complete genomes and medical records of 100,000 volunteers. Church rued the cost of the HGP and wanted to make personal genomics a possibility by reaching the holy grail of genomics: the $1,000 genome. At the time, Church's group at Harvard Medical School, were working on a new NGS technology that used “polony bead amplification of the template DNA and a common digital microscope to read fluorescent signals” (Church, 2006). The sequencing machine was launched in 2008 as the Polonator G.007 at a price tag of $150K; a 1/3 of the price of the 454 sequencer. The Polonator used of-the-shelf components and embodied an open-source platform with hackable hardware, software, and protocols. The machine was praised as the ultimate effort to make the technology as accessible and customizable as possible (ibid). Jason Bobe, the Director of Community of PGP since 2007, inspired by the DIY low-cost open-source sequencer and its potential for dropping costs cheap enough that everyone would want to have one in their garages decided to name this new garage hobby as DIYbio and registered the Internet domain DIYbio.org in 2007 (Tochetti, 2013).


The rise of the amateur biologist can be seen as the Pro-Ams of biotechnology as the technologies, skills, and knowledge required become more accessible, approachable (easy), and affordable. Biohackers are enabled by web 2.0 technologies for communication, coordination and collaboration in a decentralized and distributed fashion. The capabilities for self-learning have increased dramatically as people have free access to scientific knowledge in the form of open access scientific literature, through Massive Open Online Courses (MOOCs), or through more informal sources such as wikis and how-to instructions. The resources required for DIYbio are not solely confined to those of the cyberspace but also include physical resources of the meatspace[^17], these include things such as glassware, plasticware, chemicals and media, wetware, and hardware equipment that are obtained from DIY and institutional settings alike (Kuznetsov, Taylor, Regan, Villar, & Paulos, 2012). Low-cost tools can be obtained from off-the-shelf components or from repairing, repurposing, reverse engineering, or designing their own tools and usually share the instructions online. Many second-hand source their equipment, either bought or donated from universities and companies. Cheap equipment results from the ‘leftovers’ of bankrupt biolabs or from the rapid turnover of equipment in established biolabs (Wolinsky, 2009). This turnover results from the rapid advancements in molecular biology techniques which have not only dropped costs, they have plummeted at an even faster rate than Moore's Law (see Illustration 2.1).

#### 2.4.3 Homebrew Biotech
Jason Bobe would meet with Mackenzie Cowell in 2008 at a co-working space in Boston (Tochetti, 2013). Cowell had been working at iGEM before he quit because he “wasn't learning new things” (Bousted, 2008). Cowell sold his car for seed money and together with Bobe founded DIYbio.org which in their own words is an organization that aims to be “An Institution for the Do-it-Yourself Biologist” and established the DIYbio mailing list (Google Groups platform) which currently holds over 3,700 members and over 4,700 topics discussed. Bobe and Cowell called for the first DIYbio meeting to discuss the future of amateur biology. Around 25 biotech enthusiasts gathered at the Irish Pub in Cambridge to discuss biotechnology as a serious hobby “Can DIYbio.org be the Homebrew Computer Club of biology?” they asked (Bobe, 2008). Along these lines DIYbio represents a biological genre of the computer hacker: the biohacker (for the breakdown of biohacking see §2.4.4).


DIYbio started with people tinkering in the garages and kitchens of biotech enthusiasts (Bloom, 2009; Wolinsky, 2009), and eventually moved to dedicated community labs[^18] that have integrated into the hackerspace model, either by setting up biolabs in existing hackerspaces or setting up new dedicated biohackerspaces (Kuznetsov et al., 2012). The community lab of Genspace[^19], was one of the first to open its lab to the public in 2010 in Brooklyn, New York, founded in part by molecular biologist Ellen Jorgensen. Biohackerspaces finance themselves through different mechanisms, some of these include sponsorship through government and university subsidies, crowdfunding, membership fees, etc. DIYbio.org has become somewhat of the central hub for a global DIYbio network and accounts for 21 DIYbio groups in North America, 18 in Europe, 2 in Asia, 2 in Oceania, and 2 in Latin America in their website. Biohackerspaces comprise diverse sets of individuals such as scientists, designers, software developers, hobbyists, and enthusiasts, that work on a wide range of projects such as citizen science initiatives, amateur science, product development (incubators), artistic work, and educational workshops and courses (Landrain, Meyer, Perez, & Sussan, 2013). Individuals share the infrastructure provided in DIYbio labs to develop and contribute to projects out of their own interests and motivations with no expected outcomes in terms of market potentials, feasibility or social worth, all they have to do is “follow safety guidelines” (Jorgensen, 2012).


The potential widespread access of synthetic biology have caused alarming concerns over biosafety and biosecurity issues (Edwards & Kelle, 2012; Schmidt, 2008), even the FBI has sponsored several conferences since 2009 as outreach workshops to the DIYbio community. The conferences have been successful for building a positive dialogue and bringing awareness for biosecurity and biosafety issues (Jefferson, 2013). Cowell has expressed that building a relationship with the FBI feels counterintuitive but is important “If we're going to walk the walk, we have to be able to talk to the FBI” (Lempinen, 2011). The DIYbio community has also taken a pro-active stand towards good practices. Two regional networks, the European and the North American drafted each a code of ethics in 2011 that expressed commonality calling for open-access, transparency, safety, education, responsibility towards living beings and the environment, and for only peaceful purposes (an insightful comparison is done by Eggleson, 2014).


#### 2.4.4 The Biohacker
The Homebrew ‘Biotechnology’ Club was envisioned before Bobe in 1988 by Michael Schrage in his article “Playing God in Your Basement” in The Washington Post. Schrage makes a comparison between the homebrew hobbyists and the ‘artistic’ hacker subculture that started the personal computer revolution and suggests a similar “technology subculture could grow around DNA just as one did for silicon software”. He named this new hacker genre the bio-hacker[^20]. The label biohacker has been broadly adopted by DIYbio and by other groups that adhere to other types of biohacking like cyborg hacking (grinders) or sleep and diet hacking (Quantified Self)[^21] (see Box 2.3).

<div class="ui secondary segment" markdown="1">
**Box 2.3: Biohacker Flavors**

At least two types of self-proclaimed biohackers can be distinctly discerned from the general brand of DIYBio based on their particular interests. These groups perform self-biohacking to extend and enhance human capacities and many subscribe to the transhumanist philosophy of transforming the human condition through technologies. Self-called grinders perform practical, and sometimes extreme DIY body-enhancements with electronic hardware through body-modification and self-surgery. They are also interested in the use of nootropics and drugs to improve mental and physical functions. The other branch of biohackers can be distinguished by their extensive effort to self-measure and monitor behavioral, physical, biological and genetic metrics for self-knowledge and improvement and fall under the Quantified Self (QS) movement. Some of these groups extend to the DIYbio group and vice versa as well. What unifies these groups is the idea of hacking biological systems; of trying to understand how something works by experimentation (hacking) and they are sharing their hacks with others.
</div>

In the DIYbio/FAQ wiki page they answer Who is a “biohacker”? and reference to the hacker subculture; the homebrew computer club, free software, the hacker ethic, and DIY enthusiasts. It also notes that biohacker “might be somewhat related to biopunk”. The term biopunk originates as a science fiction subgenre of cyberpunk, both of which involve narratives of dystopian and dreary futures of high (bio)tech and a subversive culture (punks and hackers) that struggle against the social control of oppressive governments or megacorporations. The term biopunk therefore accompanies meanings of the (cyber)punk ideology and its critiques towards neo-liberalism, late capitalism, and individualistic consumer society, however these philosophies are not necessarily expressed (Schmeink, 2011), as is the case of Marcus Wohlsen's book Biopunk (2011), in which he uses biopunk interchangeably with biohackers to refer to amateur biologists and DIYbio to refer to DIYbio.org Another example is Meredith Patterson in A Biopunk Manifesto (2010) where she claims:

> As biohackers it is our responsibility to act as emissaries of science, creating new scientists out of everyone we meet (…) We the biopunks are dedicated to putting the tools of scientific investigation into the hands of nyone who wants them.

Overall the labels DIYbio, biopunk, biohacking, and amateur biology can be portrayed as the same thing[^22] (Alper, 2009; Bloom, 2009; Frushkin et al., 2013; Whalen, 2009). Biohackers have been characterized for their mode of exploration which relies on understanding biology by making (Delgado, 2013; Roosth, 2010). Moreover, by comparing themselves to computer hackers and open-source software they transfer meanings of the right to access, the right to use, and the right to modify (biological) things (Delgado, 2013, p. 66). Sophia Roosth argues that biohackers beyond trying to “democratize” biology they aim to make it “quotidian, personal, apprehensible” (2010, p. 105)—or as Mac Cowell later responded, they want to ‘domesticate’ biology (100 ideas, 2009). Roosth continues to say “This is biology as a mode of political action, in which practitioners frame doing biological research as a right rather than a privilege” (2010, p. 105).


According to Ana Delgado et al. (2013) and Alessandro Delfanti (2013) biohackers can be understood as a reaction to the current post-academic model of science which is commissioned, managed, and increasingly privatized which in their own views replaces individual curiosity and creativity. Delgado (2013) claims that herein lies the difference between institutional biology and amateur biology; “a renewed enthusiasm for exploration and discovery”. Christopher Kelty (2010) characterizes the (bio)hacker as someone who takes pleasure in understanding and modifying a system and values openness and sharing. Kelty compares the hackers with other figures in participatory biology by emphasizing that hackers work together, not alone, unlike the outlaws who take delight themselves in solely demystifying and bringing access to biology, or the Victorian Gentlemen scientists, who are well-connected eccentric intellectuals that pursue knowledge on an aesthetic and pure intellectual principle.


For Delfanti (2013), biohackers make biology ‘hackable’[^23] in several ways: First, hacking is the ultimate motive and requirement; you don't need a PhD, you just have to be curious and share your knowledge. Second, biohackers understand biology as programmable information which can be made standardized and modularized to make it cheaper and more accessible. Third, they open up community labs beyond the exclusive domain of Big Bio[^24]. Fourth, they entrepreneur in the new business models of the open-source development model. Biohackers are enabled by and foster the grassroots CBPP model of distributed and decentralized open production of common goods that challenge the monopoly of top-down, proprietary ‘Big Bio slow giants’. Thus Delfanti maintains the biohacker as the direct translation of hacking into the realm of biology:

> DIYbio embodies different faces of hacking such as openness in data and knowledge sharing as well as openness of the doors of scientific institutions, but also rebellion, hedonism, passion, communitarian spirit, individualism and entrepreneurial drive, distrust for bureaucracies.

Furthermore, Delfanti recognizes that biohackers, much like the hackers (G. Coleman, 2004; Kelty,
2008), value their craft as social and creative expressions and often deny their political intentions. Nevertheless hackers and their FOSS philosophy has extended into the wider publics and has sparked “a commons movement, centered on the idea of creating public goods to reinvigorate democratic principles” (G. Coleman, 2004, p. 514).

[^3]: Libre is sometimes used to supplement the word Free to emphasize that it refers to Freedom and not to Free of Charge as in Gratis. Stallman (2007) argues that FLOSS is better suited as a neutral term to encompass both FS and OSS.

[^4]: Note this norm does not explicitly include gender and although the hacker culture promotes openness and inclusion there is a substantial gender gap in hacker communities. Unfortunately this discussion is out of the scope of this thesis but a review on women's exclusion from FOSS-like communities can be found in “Free as in sexist?” Free culture and the gender gap by Joseph Reagle (2012)

[^5]: ‘Toffleresque’ refers to Alvin Toffler's (1981) book The Third Wave where he describes the technological history of societies in three successive waves: agricultural, industrial, and information-based.

[^6]: The most popular term used is DIY for Do-it-Yourself, but many also use the terms DIT (Do-it-Together), DIWO (Do-It-With-Others), or even DIO (Do-it-Ourselves) to imply that DIY is really a collaborative effort.

[^7]: Examples: Maker Faire in Rome, IT, FabLab Festival in Toulouse, FR, Kids Hacker Camp in Nairobi, KE. Hackathons are events in which people from varying disciplines come together, form teams and focus on prototyping a solution or idea with digital technologies in a range of different spaces such as academia, engineering, music, fashion, government, and so forth (Briscoe & Mulligan, 2014).

[^8]: OHL like the TAPR OHL and the CERN OHL follow the philosophy of FLOSS however hardware is considered ‘useful’ work so it is protected under patents and not copyright as ‘creative’ works. Thus the hardware in reality is released into the public domain where anyone can manufacture it without permission and only the design and documentation files are protected under copyleft licenses.

[^9]: As of 2012 about a third of the world's population has been online according to the Internet World Stats.
The Digital Divide is of great concern as it affects economic and social inclusion, however it is not
addressed in this paper but is elsewhere, refer to The Digital Divide by Pippa Norris (2001).

[^10]: The term was coined by Dougherty to denote a new generation of the Web that is user-centric—users create content with tools such as wikis, blogs, social networking sites, video hosting sites, etc.

[^11]: Or as Stallman humorously calls them Imposed Monopoly Privileges (IMPs) (Stallman, 2004)

[^12]: An economy describes the activities related to the production and exchange of goods and a gift represents these social exchanges. Gifts are not subject to the cost-benefit reasoning and calculated pricing of the market but instead rely on the rule of reciprocity or altruism.

[^13]: Ziman (2002) has replaced the Organized in CUDOS for Originality to describe how science favors innovative approaches and address new problems; it is the counternorm of Expert in the PLACE norms.

[^14]: The PCR (Polymerase Chain Reaction) is a thermo-chemical reaction (carried out in a thermo-cycler) in which heat is applied to a DNA molecule to split the two strands of the DNA, it is then cooled down to the polymerase enzyme's optimal temperature that replicates the two strands of DNA (1 DNA → 2 DNAs). This process cycles until sufficient amount of DNA is replicated for analysis. This technology is extensively used in molecular biology for DNA sequencing, DNA cloning, genetic diagnostics, gene analysis, etc.

[^15]: See  Synthetic Biology Explained for a crash-course on synbio.

[^16]: Craig Venter is one of the most influential and controversial characters in genomics and synthetic biology. For one, Venter tried to compete with the HGP through the private sector (Celera) which intended to profit by charging a subscription fee to a value-added database of genomic data. The public consortium published the human genome first. For more refer to The Genome War (2007) by James Shreeve. Venter is also one of the inventors of the first self-replicating bacterial cell.

[^17]: Meatspace is the world outside of cyberspace; the world of flesh and blood. The term originated from
cyberpunk novels.

[^18]: In a survey conducted by the Wilson Center, they determined that about 90% of DIYbio'ers work in group spaces rather than alone in their homes (Frushkin, Kuiken, & Millet, 2013).

[^19]: For a look into what a biohackerspace looks and ‘feels’ like watch  Visit to Genspace by Make magazine

[^20]: This is the earliest citation according to Word Spy of the word biohacker. Excerpts from the article can
also be found in Afflictor.

[^21]: These variations can hold different values, goals and conflicts and are thus not included in this study.

[^22]: A discussion over a Wikipedia cleanup in the DIYbio mailing list was split, some agreed on them being roughly the same, while other strongly maintained that these labels have different meanings and therefore bring distinct imaginaries that depend on the historical and cultural origins of the words.

[^23]: Or in other words, it is changing, modifying, remixing the system.

[^24]: Big Bio in reference to “the ensemble of big corporations, global universities, and international and governmental agencies that compose the economic system of current life sciences” (Delfanti, 2013, p. 6).
